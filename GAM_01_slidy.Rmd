---
title: "lalala Generalized Additive Model (GAM)"
author: "Hsiao-Hang & Oscar"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  slidy_presentation:
    highlight: pygments
    incremental: no
    widescreen: yes
  beamer_presentation:
    highlight: pygments
    incremental: no
  ioslides_presentation:
    highlight: pygments
    incremental: no
institute: Institute of Oceanography, National Taiwan University
subtitle: study group
fontsize: 14 pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# General form

- **Generalized additive model** is a generalized linear model with a linear predictor, involving a sum of smooth functions of covariates.

$g(u_i) = X^*_i \theta + f_1(x_{1i}) + f_2(x_{2i}) + f_3(x_{3i}, x_{4i}) + ...$ (eqn. 3.1)

- GAM specifies the model with **smooth functions** rather than detailed parametric relationships.


# Univariate smooth functions $f$

$y_i = f(x_i) + \epsilon_i$ (eqn. 3.2)

$y_i$ : response variable \
$f$ : **smooth function** \
$x_i$ : covariate \
$\epsilon_i$ : i.i.d. $N(0, \sigma^2)$ variable (error) \

- Represent the smooth function *f* in a way that (eqn. 3.2) becomes a linear model. 
- The smooth function *f* can then be expressed as some \
  *basis functions* $(b)$ with parameter $\beta$
  
$f(x_i) = \sum_{j=1}^{q} b_j(x)\beta_j$


# Simple example of smooth function: 4<sup>th</sup> order polynomial basis

$f(x) = \beta_1 + x\beta_2 + x^2\beta_3 + x^3\beta_4 + x^4\beta_5$

![](Ch3_fig/Fig_3_1.png){width=600px}

![](Ch3_fig/Fig_3_2.png){width=600px}

- Polynomial basis can be problematic...**spline bases** are more common


# Cubic spline basis

- A univariate function represented using a cubic spline. 

- A cubic spline is a curve, made up of sections of cubic polynomial, joined together so that they are continuous in value as well as first and second derivatives 

![](Ch3_fig/Fig_3_3.png){width=600px}

- A cubic spline basis consist of *one constant* and *one 1<sup>st</sup> order (linear)* term and $k$ cubic polynomials
  - **$k$ is the number of knots**
  - knots are evenly spaced or at the quantile of distribution of x ($x_i^*; i = 1 \sim k$)
  - $k+2$ is the rank of the cubic spline function

# 

A cubic spline basis can be expressed as...

$b_1(x)=1, b_2(x)=x,...,b_{i+2}(x,x_k^*)$

- $b_{i+2}(x,x_k^*)$ is a cubic polynomial written in **eqn 3.4** 

- Each cubic polynomial will peak at the knot
  - e.g. The four cubic polynomials when $k = 4$ \
```{r, fig.height = 4}
rk <- function(x, z) { 
  ((z-0.5)^2-1/12) * ((x-0.5)^2-1/12)/4- ((abs(x-z)-0.5)^4-(abs(x-z)-0.5)^2/2+7/240)/24
}

lala <- seq(0.01, 1, 0.01)
knot <- 1:4/5
plot(rk(lala, knot) ~ lala)
```

The peak of the cubic polynomial is *conceptually* the weight when fitting the $y$

# Example of cubic spline bases

rank 5 cubic regression spline basis 

![](Ch3_fig/Fig_3_4.png){width=600}


![](Ch3_fig/Fig_3_5.png){width=600}


# Working example

Data
```{r, echo = TRUE}
size <- c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13,2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98)
wear <- c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9,3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7)

x <- size - min(size)
x <- x/max(x)
```

Function for generating the model matrix
```{r, echo = TRUE}
spl.X <- function(x,xk) # set up model matrix for cubic penalized regression spline 
  { q <- length(xk) + 2 # number of parameters 
    n <- length(x) # number of data 
    X <- matrix(1, n, q) # initialized model matrix 
    X[,2] <- x  # set second column to x
    X[,3:q] <- outer(x, xk, FUN = rk) # and remaining to R(x,xk) 
    X
}
```


#

Results...\
```{r}
xk <- 1:4/5 # choose some knots
X <- spl.X(x,xk) # generate model matrix 
mod.1 <- lm(wear~X-1) # fit model
xp <- 0:100/100 # x values for prediction 
Xp <- spl.X(xp,xk) # prediction matrix 

plot(x,wear,xlab="Scaled engine size",ylab="Wear index")
lines(xp,Xp%*%coef(mod.1)) # plot fitted spline
```


# Knots determine the smoothness

- How many knots?

- **Penalized regression splines**

  Minimize the following...$||y - X\beta|| + \boldsymbol{\lambda \int_{0}^{1}[f''(x)]^2dx}$ \
  
  High $\lambda$ penalizes high wiggliness ($\int_{0}^{1}[f''(x)]^2dx$)
  
  Fig. 3.8
  
- Still need to determine a number of knots


# To determine $\lambda$

- Ordinary cross validation (OCV) \
  Leave one data point at a time when doing cross validation

- Generalized cross validation (GCV) \
  Use *influence matrix* to estimate the deviance when leaving one data point out
  Fig. 3.10
  

# Additive model (more than one $x$)

$y_i = f_1(x_{1i}) + f_2(x_{2i}) + \epsilon_i$

All calculations are conceptually the same as mentioned above.


# Generalized additive model

Due to the link function, penalized likelihood is being maximized. 

In practice, **penalized iteratively re-weighted least square (P-IRLS)** is implemented. 
